{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-05T09:51:23.387935Z",
     "start_time": "2023-08-05T09:51:23.382597Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "\n",
    "from langchain.chat_models.human import HumanInputChatModel\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# set OpenAI API key\n",
    "load_dotenv(dotenv_path=\"../../.env\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# set OpenAI API Key to langchain and set temperature to 0.9\n",
    "# chat = ChatOpenAI(model_name=\"text-davinci-003\", openai_api_key=openai.api_key, temperature=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T09:51:24.310520Z",
     "start_time": "2023-08-05T09:51:24.306179Z"
    }
   },
   "id": "f368c3d739842a64"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# pseudo Chat Model class that can be used for testing, debugging, or educational purposes. \n",
    "# This allows you to mock out calls to the Chat Model and simulate how a human would respond if they received the messages.\n",
    "tools = load_tools([\"wikipedia\"])\n",
    "\n",
    "llm = HumanInputChatModel()\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T09:51:25.533257Z",
     "start_time": "2023-08-05T09:51:25.447656Z"
    }
   },
   "id": "87dcf5c5118c353a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new AgentExecutor chain...\u001B[0m\n",
      "\n",
      " ======= start of message ======= \n",
      "\n",
      "\n",
      "type: system\n",
      "data:\n",
      "  content: \"Answer the following questions as best you can. You have access to the following tools:\\n\\nWikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Wikipedia\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\"\n",
      "  additional_kwargs: {}\n",
      "\n",
      "======= end of message ======= \n",
      "\n",
      "\n",
      "\n",
      " ======= start of message ======= \n",
      "\n",
      "\n",
      "type: human\n",
      "data:\n",
      "  content: 'What is Bocchi the Rock?\n",
      "\n",
      "\n",
      "    '\n",
      "  additional_kwargs: {}\n",
      "  example: false\n",
      "\n",
      "======= end of message ======= \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43magent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is Bocchi the Rock?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/base.py:258\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 258\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    259\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    260\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    261\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[1;32m    262\u001B[0m )\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/base.py:252\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    246\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    247\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    248\u001B[0m     inputs,\n\u001B[1;32m    249\u001B[0m )\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 252\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    255\u001B[0m     )\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/agents/agent.py:1029\u001B[0m, in \u001B[0;36mAgentExecutor._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;66;03m# We now enter the agent loop (until it returns something).\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_continue(iterations, time_elapsed):\n\u001B[0;32m-> 1029\u001B[0m     next_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_take_next_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname_to_tool_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolor_mapping\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1032\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1033\u001B[0m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1034\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1035\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1036\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(next_step_output, AgentFinish):\n\u001B[1;32m   1037\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_return(\n\u001B[1;32m   1038\u001B[0m             next_step_output, intermediate_steps, run_manager\u001B[38;5;241m=\u001B[39mrun_manager\n\u001B[1;32m   1039\u001B[0m         )\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/agents/agent.py:832\u001B[0m, in \u001B[0;36mAgentExecutor._take_next_step\u001B[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001B[0m\n\u001B[1;32m    829\u001B[0m     intermediate_steps \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_prepare_intermediate_steps(intermediate_steps)\n\u001B[1;32m    831\u001B[0m     \u001B[38;5;66;03m# Call the LLM to see what to do.\u001B[39;00m\n\u001B[0;32m--> 832\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplan\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m        \u001B[49m\u001B[43mintermediate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    835\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    836\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    837\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m OutputParserException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    838\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_parsing_errors, \u001B[38;5;28mbool\u001B[39m):\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/agents/agent.py:456\u001B[0m, in \u001B[0;36mAgent.plan\u001B[0;34m(self, intermediate_steps, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Given input, decided what to do.\u001B[39;00m\n\u001B[1;32m    445\u001B[0m \n\u001B[1;32m    446\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;124;03m    Action specifying what tool to use.\u001B[39;00m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    455\u001B[0m full_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_full_inputs(intermediate_steps, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 456\u001B[0m full_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfull_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_parser\u001B[38;5;241m.\u001B[39mparse(full_output)\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/llm.py:252\u001B[0m, in \u001B[0;36mLLMChain.predict\u001B[0;34m(self, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    238\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \n\u001B[1;32m    240\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 252\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key]\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/base.py:258\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 258\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    259\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    260\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    261\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[1;32m    262\u001B[0m )\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/base.py:252\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001B[0m\n\u001B[1;32m    246\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    247\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    248\u001B[0m     inputs,\n\u001B[1;32m    249\u001B[0m )\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    251\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 252\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    253\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    254\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    255\u001B[0m     )\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/llm.py:92\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     89\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m     90\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     91\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m---> 92\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chains/llm.py:102\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001B[39;00m\n\u001B[1;32m    101\u001B[0m prompts, stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_prompts(input_list, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    104\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chat_models/base.py:401\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    395\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    398\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    399\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    400\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 401\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chat_models/base.py:296\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    295\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e)\n\u001B[0;32m--> 296\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    297\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    298\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    300\u001B[0m ]\n\u001B[1;32m    301\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chat_models/base.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    284\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    285\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 286\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    292\u001B[0m         )\n\u001B[1;32m    293\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chat_models/base.py:433\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    430\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    431\u001B[0m     )\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[0;32m--> 433\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    435\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    436\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chat_models/human.py:109\u001B[0m, in \u001B[0;36mHumanInputChatModel._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03mDisplays the messages to the user and returns their input as a response.\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    ChatResult: The user's input as a response.\u001B[39;00m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmessage_func(messages, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmessage_kwargs)\n\u001B[0;32m--> 109\u001B[0m user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations\u001B[38;5;241m=\u001B[39m[ChatGeneration(message\u001B[38;5;241m=\u001B[39muser_input)])\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/chat_models/human.py:57\u001B[0m, in \u001B[0;36m_collect_yaml_input\u001B[0;34m(messages, stop)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Try to parse the input string as YAML\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 57\u001B[0m     message \u001B[38;5;241m=\u001B[39m \u001B[43m_message_from_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43myaml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msafe_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mStringIO\u001B[49m\u001B[43m(\u001B[49m\u001B[43myaml_string\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m message \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m HumanMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Code/venv/openai-research/lib/python3.11/site-packages/langchain/schema/messages.py:233\u001B[0m, in \u001B[0;36m_message_from_dict\u001B[0;34m(message)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_message_from_dict\u001B[39m(message: \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m--> 233\u001B[0m     _type \u001B[38;5;241m=\u001B[39m \u001B[43mmessage\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m HumanMessage(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmessage[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[0;31mTypeError\u001B[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "agent(\"What is Bocchi the Rock?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T09:53:44.514910Z",
     "start_time": "2023-08-05T09:53:34.138082Z"
    }
   },
   "id": "e12bf2bb8d2e26d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3608081031680e1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
