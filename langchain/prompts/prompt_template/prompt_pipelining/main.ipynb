{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-30T12:13:35.572139Z",
     "start_time": "2023-07-30T12:13:34.847028Z"
    }
   },
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# set OpenAI API key\n",
    "load_dotenv(dotenv_path=\"../../../.env\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# set OpenAI API Key to langchain and set temperature to 0.9\n",
    "llm = OpenAI(openai_api_key=openai.api_key, temperature=0.9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-30T12:13:49.524811Z",
     "start_time": "2023-07-30T12:13:49.517580Z"
    }
   },
   "id": "7d2dda794e668014"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Why don't scientists trust atoms playing sports?\\n\\nBecause they make up everything!\""
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt Pipeline : user-friendly interface for composing different parts of prompts together.\n",
    "# String Prompt Pipeline\n",
    "prompt = (\n",
    "        PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "        + \", make it funny\"\n",
    "        + \"\\n\\nand in {language}\"\n",
    ")\n",
    "\n",
    "# OR\n",
    "# prompt = PromptTemplate(input_variables=['language', 'topic'], output_parser=None, partial_variables={}, template='Tell me a joke about {topic}, make it funny\\n\\nand in {language}', template_format='f-string', validate_template=True)\n",
    "\n",
    "prompt.format(topic=\"sports\", language=\"spanish\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = LLMChain(llm=model, prompt=prompt)\n",
    "chain.run(topic=\"sports\", language=\"english\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-30T12:13:51.505995Z",
     "start_time": "2023-07-30T12:13:50.299035Z"
    }
   },
   "id": "e26793d4258a86a2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'SystemMessage' and 'HumanMessage'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Chat Prompt Pipeline\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# possibly bug? \u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# https://github.com/langchain-ai/langchain/issues/8472\u001B[39;00m\n\u001B[1;32m      4\u001B[0m prompt \u001B[38;5;241m=\u001B[39m SystemMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are a nice pirate\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m new_prompt \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m----> 6\u001B[0m         \u001B[43mprompt\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mHumanMessage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mhi\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{input}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      9\u001B[0m )\n\u001B[1;32m     10\u001B[0m new_prompt\u001B[38;5;241m.\u001B[39mformat_messages(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi said hi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m model \u001B[38;5;241m=\u001B[39m ChatOpenAI()\n",
      "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'SystemMessage' and 'HumanMessage'"
     ]
    }
   ],
   "source": [
    "# Chat Prompt Pipeline\n",
    "# possibly bug? \n",
    "# https://github.com/langchain-ai/langchain/issues/8472\n",
    "prompt = SystemMessage(content=\"You are a nice pirate\")\n",
    "new_prompt = (\n",
    "        prompt\n",
    "        + HumanMessage(content=\"hi\")\n",
    "        + \"{input}\"\n",
    ")\n",
    "new_prompt.format_messages(input=\"i said hi\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "chain = LLMChain(llm=model, prompt=new_prompt)\n",
    "chain.run(\"i said hi\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-30T12:18:13.110055Z",
     "start_time": "2023-07-30T12:18:13.096572Z"
    }
   },
   "id": "ed52f1f9c9000b38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "884d6afcf686bd99"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
